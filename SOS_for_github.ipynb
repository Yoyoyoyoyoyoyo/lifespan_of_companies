{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using states' Secretary of State data to learn about organizations\n",
    "\n",
    "\n",
    "## Outline:\n",
    "    1. Scrape some data and load it into Elasticsearch\n",
    "    2. Use Kibana somehow?\n",
    "    3. Query Elasticsearch for data to put into a pandas dataframe\n",
    "    4. Prep the dataframe by adding new variables\n",
    "    5. Make models and learn something new about companies in Rhode Island\n",
    "\n",
    "### Before starting, make sure you have ready:\n",
    "    - Elasticsearch (installed and running) (both the program & the Python package)\n",
    "    - Tor (installed and running)\n",
    "    - bs4, aka BeautifulSoup\n",
    "    - pandas\n",
    "    - pysocks isn't imported, but it must be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import collections\n",
    "import elasticsearch.helpers\n",
    "from pandas.io.json import json_normalize\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: scrape some data\n",
    "\n",
    "### Scraping data can be illegal! I'm not recommending you do it, just showing you how easy it is to do. If you're interested in the analysis, you can buy this full database from the Rhode Island Secretary of State for fifty bucks.\n",
    "\n",
    "\n",
    "### We're using Tor just to show how to use Tor. Unless you're scraping the dark web, using Tor is pretty impractical. Tor is orders of magnitude slower than normal requests.\n",
    "\n",
    "### The data we collect is being stored in Elasticsearch. You could keep it around in memory, but it takes a long time to scrape data, so it's better to save it to a database so you can quickly use it again in the future if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Set up your proxies to use Tor\n",
    "    session.proxies = {'http': 'socks5://localhost:9150',\n",
    "                       'https': 'socks5://localhost:9150'}\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_and_except_loop(soup, data_dict, value, prefix, value_not_text=False):\n",
    "    \"\"\"A short loop to keep things a little DRYer\"\"\"\n",
    "    if value_not_text:\n",
    "        try:\n",
    "            data_dict[value] = soup.find(id=prefix + str(value)).value\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    else:\n",
    "        try:\n",
    "            data_dict[value] = soup.find(id=prefix + str(value)).text\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After inspecting the web page we're going to be grabbing, I got the names of the worthwhile fields:\n",
    "VALUES_TO_EXTRACT = [\"EntityName\",\n",
    "                    \"EntityType\",\n",
    "                    \"OrganisationDate\", # Yes, this is misspelled in the HTML\n",
    "                    \"EffectiveDate\",\n",
    "                    \"InactiveDate\",\n",
    "                    \"PrincipleStreet\",\n",
    "                    \"PrincipleCity\",\n",
    "                    \"PrincipleState\",\n",
    "                    \"PrincipleZip\",\n",
    "                    \"PrincipleCountry\",\n",
    "                    \"ResidentAgentFlag\",\n",
    "                    \"ConsentFlag\",\n",
    "                    \"ResidentAgentName\",\n",
    "                    \"ResidentStreet\",\n",
    "                    \"ResidentCity\",\n",
    "                    \"ResidentState\",\n",
    "                    \"ResidentZip\",\n",
    "                    \"ResidentCountry\"]\n",
    "\n",
    "def parse_rhode_island_entity_summary(session):\n",
    "    \"\"\"When given Rhode Island's Entity Summary page, this builds a dictionary of\n",
    "        all the noteworthy data on the webpage\"\"\"\n",
    "    data_dict = {}\n",
    "    soup = BeautifulSoup(session._content)\n",
    "\n",
    "    # This value includes the words \"Identification Number: \". Clear that useless text.\n",
    "    # Also, if there is no ID number present, then there's no other information present either,\n",
    "    # and there's no need to continue selecting data.\n",
    "    try:\n",
    "        data_dict[\"IDNumber\"] = soup.find(id=\"MainContent_lblIDNumber\").text[-9:]\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "    for item in VALUES_TO_EXTRACT:\n",
    "        try_and_except_loop(soup, data_dict, item, \"MainContent_lbl\")\n",
    "    try_and_except_loop(soup, data_dict, \"Comments\", \"MainContent_txt\")\n",
    "    try_and_except_loop(soup, data_dict, \"NIACS\", \"MainContent_txt\", value_not_text=True)\n",
    "\n",
    "    # The length of the list of directors & officers varies by company\n",
    "    d_and_o_list = []\n",
    "    table = soup.find(id=\"MainContent_grdOfficers\")\n",
    "    try:\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            col = row.find_all('td')\n",
    "            d_and_o_list += [col[0].text, col[1].text, col[2].text]\n",
    "    except AttributeError:\n",
    "        # Some pages don't have information on directors & officers\n",
    "        pass\n",
    "    data_dict['d_and_o'] = d_and_o_list\n",
    "\n",
    "    # I have no idea if their data on stocks is meaningful, but just in case:\n",
    "    try:\n",
    "        stock_cols = soup.find(id=\"MainContent_grdStocks\").tbody.tr.find_all('td')\n",
    "        data_dict['stock_info'] = [stock_cols[item].text for item in range(len(stock_cols))]\n",
    "    except AttributeError:\n",
    "        # Some pages don't have information on stocks. Makes sense, since all the info isn't on corporations.\n",
    "        pass\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "\n",
    "def create_elasticsearch_index():\n",
    "    \"\"\"This should only be run once: re-running it will delete any saved data!\"\"\"\n",
    "    es.indices.delete(index=\"ri_sos\", ignore=[400, 404])\n",
    "    es_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"corporate_data\": {\n",
    "                \"properties\": {\n",
    "                    \"OrganisationDate\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"MM-dd-yyyy\"\n",
    "                    },\n",
    "                    \"EffectiveDate\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"MM-dd-yyyy\"\n",
    "                    },\n",
    "                    \"InactiveDate\": {\n",
    "                        \"type\": \"date\",\n",
    "                        \"format\": \"MM-dd-yyyy\"\n",
    "                    },\n",
    "                    \"IDNumber\": {\n",
    "                        \"type\": \"integer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }}\n",
    "    es.indices.create(index=\"ri_sos\", body=es_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_in_elasticsearch(returned_page):\n",
    "    \"\"\"Updates elasticsearch with a single row of data\"\"\"\n",
    "    try:\n",
    "        es.index(index=\"ri_sos\", doc_type=\"corporate_data\", body=json.dumps(returned_page))\n",
    "    except:\n",
    "        # Don't let the scraping stop in case some page has an unusual structure.\n",
    "        # Instead, print out the bad data & figure out the exceptions later.\n",
    "        print(\"Didn't save:\", returned_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rhode_island_data(start_at=0, records_to_get=1, create_index=False, random_subset_size=False, prior_records=False,\n",
    "                              failed_records=[]):\n",
    "    \"\"\"This function ties together connecting with Tor, scraping data, and loading it into Elasticsearch.\n",
    "        It returns a list of all records it unsuccessfully tried to grab.\"\"\"\n",
    "    URL_string = \"http://ucc.state.ri.us/CorpWeb/CorpSearch/CorpSummary.aspx?FEIN=\"\n",
    "    session = get_tor_session()\n",
    "    # Since we're already using Tor, let's pretend we're accessing the web through a browser\n",
    "    # instead of letting people see it's through a Python script:\n",
    "    headers = {'User-agent': 'Opera/1.1.2 FCS'}\n",
    "\n",
    "    ## If you want to confirm that you're using Tor, then see if your\n",
    "    ## session's IP address matches a normal request's IP address\n",
    "    # print(\"TOR IP address:\", session.get(\"http://httpbin.org/ip\").text)\n",
    "    # print(\"Your normal IP address:\", requests.get(\"http://httpbin.org/ip\").text)\n",
    "\n",
    "    # If this is your first time through, then you'll need to create an index in Elasticsearch\n",
    "    if create_index:\n",
    "        create_elasticsearch_index()\n",
    "\n",
    "    # Options for random sampling of data\n",
    "    if random_subset_size:\n",
    "        records = random.sample(range(165000), random_subset_size)\n",
    "    else:\n",
    "        records = [start_at + item for item in range(records_to_get)]\n",
    "    if prior_records:\n",
    "        records = set(records) - set(prior_records)\n",
    "\n",
    "    for record in records:\n",
    "        corp_id = str(start_at + record)\n",
    "        # The string provided should be 9 digits long, padded by prepended by 0s\n",
    "        corp_id = corp_id.rjust(9, \"0\")\n",
    "\n",
    "        returned_page = session.get(URL_string + str(corp_id), headers=headers)\n",
    "        data_dict = parse_rhode_island_entity_summary(returned_page)\n",
    "        if data_dict:\n",
    "            # Value will be false if the page scraped is blank, so no need to save anything\n",
    "            save_data_in_elasticsearch(data_dict)\n",
    "        else:\n",
    "            failed_records += [record]\n",
    "\n",
    "        # Clear cookies to make it harder to track you between requests:\n",
    "        session.cookies.clear()\n",
    "    return failed_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment & run this if it's your first time running this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we finally start collecting some data\n",
    "#### ONLY RUN THIS ONCE! OTHERWISE IT WILL DELETE YOUR INDEX, DELETING ALL OF YOUR DATA!\n",
    "# failed_records = collect_rhode_island_data(start_at=0, \n",
    "#                                           records_to_get=5,\n",
    "#                                           create_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otherwise load your previously saved list of failed records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your list of pickled failed records:\n",
    "\n",
    "#with open('failed_records', 'rb') as fp:\n",
    "#    failed_records = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhode Island has about 165,000 companies that follow the expected URL pattern. At 1 request/second, that's ~46 hours of scraping to get everything. We could speed up the process by making bulk entries into Elasticsearch rather than making individual entries, by making multiple requests at a time, or using normal requests instead of Tor, but we don't want to overwhelm the server. Always be nice to whoever has to maintain the servers you're talking to. Scraping slowly is nice.\n",
    "\n",
    "### We don't need the full database to do some analysis. Instead of scraping everything, let's randomly pull some samples and study that subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_records = collect_rhode_island_data(start_at=0,\n",
    "                                           random_subset_size=5,\n",
    "                                           failed_records=failed_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull prior_records from Elasticsearch so you don't pull them again:\n",
    "def get_prior_records(failed_records):\n",
    "    \"\"\"Return a list of all the IDNumbers saved in Elasticsearch, including any failed_records you want to exclude\"\"\"\n",
    "    records_search = elasticsearch.helpers.scan(\n",
    "        es, \n",
    "        query={\"query\": {\"match_all\": {}}, \"_source\":[\"IDNumber\"]},\n",
    "        index=\"ri_sos\",\n",
    "        doc_type=\"corporate_data\")\n",
    "\n",
    "    # Return these records, plus the failed_records we already knew about\n",
    "    return [item['_source']['IDNumber'] for item in list(records_search)] + failed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape some new records, but don't repeat anything that's already saved in Elasticsearch:\n",
    "failed_records = collect_rhode_island_data(start_at=0,\n",
    "                                           random_subset_size=10,\n",
    "                                           prior_records=get_prior_records(failed_records),\n",
    "                                           failed_records=failed_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run the above command whenever you want to add more new data to Elasticsearch. Remember to be nice about it! Don't do it in the middle of a workday, ask for 10,000 records at once, etc. Or don't do it at all because of its questionable legality. You can order the entire DB for $50 from their office."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: inspect your data in Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kibana gives you those pretty graphs in iframes. They're nice and easy to put on a web page.\n",
    "# I'll add them if I ever make a blog post about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's strange that there are records with InactiveDates from 1820 in my data. \n",
    "### Visual inspection of the data in Kibana shows that data with an InactiveDate is incredibly sparse until around 1985. I'm going to assume that Rhode Island didn't worry about putting in many inactive corporations into their corporate database when they created it. \n",
    "### It seems reasonable to exclude companies founded before 1984. However, that also puts a ceiling on the life expectancies of companies in my data. Whether they should be excluded or not depends on how you want to use this analysis. I'm going to leave them for now and view this as a study of companies that existed between 1985 and today, regardless of when they were founded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: load your data into pandas and inspect it more carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32480, 23)\n"
     ]
    }
   ],
   "source": [
    "# Let's load the database into pandas. Our actual analysis later will only use samples that\n",
    "# have an InactiveDate, so only request data with that data. We'd also want to filter out newer\n",
    "# companies to only work with companies that have had a long time to go out of business, but\n",
    "# reviewing our data in Kibana shows that our newest data is in 2007, implying that newer\n",
    "# companies have a different URL pattern than older companies. No need to filter based on date organized.\n",
    "\n",
    "data_to_load = elasticsearch.helpers.scan(\n",
    "                    es,\n",
    "                    query={\"query\": {\"exists\": {\"field\": \"InactiveDate\"}}},\n",
    "                    index=\"ri_sos\",\n",
    "                    doc_type=\"corporate_data\")\n",
    "\n",
    "# Initialize a double ended queue\n",
    "output_all = collections.deque()\n",
    "# Extend deque with iterator\n",
    "output_all.extend(data_to_load)\n",
    "only_data = [item[\"_source\"] for item in output_all]\n",
    "\n",
    "df = json_normalize(only_data)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by seeing what types of companies we have lots of data on.\n",
    "### First get the most commmon words in the names of companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inc', 18764), ('llc', 6263), ('&', 2499), ('corporation', 1647), ('of', 1365), ('the', 1315), ('company', 1293), ('co', 1147), ('realty', 1145), ('services', 1143), ('ltd', 1031), ('associates', 1019), ('corp', 962), ('island', 842), ('and', 796), ('rhode', 718), ('construction', 696), ('enterprises', 667), ('group', 647), ('incorporated', 618)]\n"
     ]
    }
   ],
   "source": [
    "# Remove all punctuation from the names of companies\n",
    "list_of_names = [row[1] for row in df['EntityName'].iteritems()]\n",
    "\n",
    "list_of_names_no_punctuation = []\n",
    "for name in list_of_names:\n",
    "    list_of_names_no_punctuation += [re.sub('\\,|\\.|\\!|\\;|\\:|\\?|', '', name)]\n",
    "\n",
    "# Break each name into a list of words so we can count the occurrence of each word\n",
    "new_list = []\n",
    "for words in list_of_names_no_punctuation:\n",
    "    for word in words.split():\n",
    "        new_list += [word.lower()]\n",
    "\n",
    "# Count the occurrence of each word:\n",
    "name_count = Counter(new_list)\n",
    "popular_names = name_count.most_common(500)\n",
    "\n",
    "# Print the 10 most popular names:\n",
    "print(popular_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inc', 18764), ('llc', 6263), ('&', 2499), ('corporation', 1648), ('service', 1638), ('of', 1365), ('the', 1315), ('company', 1293), ('co', 1148), ('realty', 1145), ('ltd', 1031), ('associate', 1021), ('corp', 969), ('island', 842), ('and', 796), ('rhode', 722), ('enterprise', 715), ('construction', 697), ('group', 648), ('incorporated', 618)]\n"
     ]
    }
   ],
   "source": [
    "# Let's make a 2nd list to see if the popularity changes much if we account for common word endings:\n",
    "list_of_names_no_endings = []\n",
    "for word in new_list:\n",
    "    if word [-1:] == 's':\n",
    "        list_of_names_no_endings += [word[:-1]]\n",
    "    elif word[-3:] == 'ing':\n",
    "        list_of_names_no_endings += [word[:-3]]\n",
    "    else:\n",
    "        list_of_names_no_endings += [word]\n",
    "\n",
    "# Count the occurrence of each word:\n",
    "name_count2 = Counter(list_of_names_no_endings)\n",
    "popular_names2 = name_count2.most_common(500)\n",
    "\n",
    "# Print the 10 most popular names:\n",
    "print(popular_names2[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After some inspection, there wasn't much difference between the two lists for names of data. At least we tried. Let's filter out some unhelpful words and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('realty', 1145), ('services', 1143), ('island', 842), ('rhode', 718), ('construction', 696), ('management', 532), ('service', 495), ('new', 469), ('properties', 423), ('development', 405), ('international', 389), ('mortgage', 353), ('systems', 353), ('england', 333), ('ri', 332), ('center', 316), ('auto', 306), ('financial', 305), ('american', 291), ('providence', 264), ('street', 264), ('home', 246), ('insurance', 233), ('investments', 229), ('communications', 210), ('entity', 207), ('business', 207), ('solutions', 205), ('not', 203), ('qualified', 200), ('into', 197), ('consulting', 195), ('capital', 195), ('sales', 190), ('**merged', 189), ('builders', 189), ('marketing', 186), ('east', 185), ('agency', 182), ('holdings', 180), ('restaurant', 180), ('products', 178), ('real', 177), ('design', 176), ('newport', 176), ('investment', 174), ('estate', 173), ('foundation', 173), ('industries', 171), ('family', 162), ('building', 157), ('health', 153), ('state', 153), ('t', 152), ('north', 150), ('care', 146), ('national', 144), ('ocean', 141), ('leasing', 140), ('america', 138), ('bay', 138), ('llp', 137), ('technologies', 131), ('hill', 131), ('jewelry', 131), ('usa', 127), ('marine', 124), ('consultants', 124), ('medical', 124), ('electric', 123), ('west', 121), ('property', 115), ('trucking', 114), ('cleaning', 112), ('south', 111), ('county', 111), ('partners', 110), ('md', 110), ('food', 108), ('productions', 107)]\n"
     ]
    }
   ],
   "source": [
    "# There are a lot of unhelpful words in this list. Let's filter out some of the common ones:\n",
    "boring_names = ['inc', 'llc', '&', 'corporation', 'the', 'of', 'company', 'associate', 'co', 'ltd', 'corp', 'and', 'enterprise',\n",
    "               'association', 'group', 'associates', 'enterprises', 'incorporated', 'a', 'limited', 'm', 'r', 'c', 'partnership',\n",
    "               'club', 'in', '', 'd', 'for', 'b', 'lp', 'e', 'h', 'g', 'k', 'p', 'l', '-', 'an', 's', 'i', 'f', 'j']\n",
    "popular_and_not_boring = [item for item in popular_names if item[0] not in boring_names]\n",
    "print(popular_and_not_boring[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifespan of businesses: a quick inspection\n",
    "### Our goal is to see how long businesses in different industries stay in business. We don't have a variable for length in business yet, so let's make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Overflow in int64 addition",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-50bc2e13df14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Add the variable 'time_in_business' when we can calculate it:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'InactiveDate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'time_in_business'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'InactiveDate'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'OrganisationDate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# We're trying to predict time_in_business, so let's remove all rows without a value for that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[1;31m# test_dt64_series_add_intlike, which the index dispatching handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;31m# specifically.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_to_index_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m             return construct_result(left, result,\n\u001b[0;32m   1554\u001b[0m                                     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mdispatch_to_index_op\u001b[1;34m(op, left, right, index_class)\u001b[0m\n\u001b[0;32m   1189\u001b[0m         \u001b[0mleft_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNullFrequencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[1;31m# DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[1;31m# test_dt64_series_add_intlike, which the index dispatching handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;31m# specifically.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1552\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_to_index_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1553\u001b[0m             return construct_result(left, result,\n\u001b[0;32m   1554\u001b[0m                                     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mdispatch_to_index_op\u001b[1;34m(op, left, right, index_class)\u001b[0m\n\u001b[0;32m   1189\u001b[0m         \u001b[0mleft_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mNullFrequencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m         \u001b[1;31m# DatetimeIndex and TimedeltaIndex with freq == None raise ValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\ops.py\u001b[0m in \u001b[0;36mrsub\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrsub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mright\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\indexes\\datetimelike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;31m# dispatch to ExtensionArray implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_unwrap_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrap_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m             \u001b[1;31m# DatetimeIndex, ndarray[datetime64]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1275\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sub_datetime_arraylike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1276\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_period_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m             \u001b[1;31m# PeriodIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36m_sub_datetime_arraylike\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[0mother_i8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masi8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         new_values = checked_add_with_arr(self_i8, -other_i8,\n\u001b[1;32m--> 724\u001b[1;33m                                           arr_mask=self._isnan)\n\u001b[0m\u001b[0;32m    725\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hasnans\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_isnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aubre\\desktop\\github_math\\git_math\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mchecked_add_with_arr\u001b[1;34m(arr, b, arr_mask, b_mask)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_raise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Overflow in int64 addition\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Overflow in int64 addition"
     ]
    }
   ],
   "source": [
    "# How many companies have out-of-business dates?\n",
    "df = df.assign(time_in_business=pd.Series(np.nan))\n",
    "\n",
    "# Convert date columns to the date format\n",
    "df['InactiveDate'] = pd.to_datetime(df['InactiveDate'])\n",
    "df['OrganisationDate'] = pd.to_datetime(df['OrganisationDate'])\n",
    "\n",
    "# Add the variable 'time_in_business' when we can calculate it:\n",
    "df.loc[df['InactiveDate'] != np.nan, 'time_in_business'] = (df['InactiveDate'] - df['OrganisationDate']).dt.days\n",
    "\n",
    "# We're trying to predict time_in_business, so let's remove all rows without a value for that\n",
    "df = df[df.time_in_business.notnull()]\n",
    "print(\"Returned shape:\", df.shape)\n",
    "\n",
    "print(df.time_in_business.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with negative values for time_in_business if you have them in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is there a negative value for time_in_business? Your sample may not have one, but mine did.\n",
    "\n",
    "#weird_row = df.loc[df.time_in_business == df.time_in_business.min()]\n",
    "#print(weird_row.IDNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On manual inspection, Rhode Island's records do in fact show at least one company as having been \n",
    "# dissolved before it was created. That's very strange. \n",
    "# Manually inspect & delete any records with negative lifespans.\n",
    "\n",
    "#weird_rows = df.loc[df.time_in_business < 0]\n",
    "#print(weird_rows.IDNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I'm dropping the row that appeared in my data. Your data will most likely have a different row to drop.\n",
    "# Don't run this cell without inspecting your own data!\n",
    "\n",
    "#df.drop([weird_rows.index[0]], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max time_in_business number I have is 61626 days, or 169 years! Does Rhode Island really have that much data?\n",
    "too_long = df.loc[df.time_in_business > 36500]\n",
    "print(too_long[[\"IDNumber\", \"time_in_business\"]])\n",
    "\n",
    "# I manually inspected a few of these in my data, and they were legit. An old bank, an old cemetery, etc.\n",
    "# There's a good argument to be made for excluding all businesses founded so long ago, but we're not going to deal with that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's graph the distribution to see what it looks like\n",
    "plt.hist(df.time_in_business, bins = 365,\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "plt.axis([0, 10000, 0, 700])\n",
    "plt.title('Binwidth = 1 year', size = 30)\n",
    "plt.xlabel('Days in business', size = 22)\n",
    "plt.ylabel('# of companies', size= 22)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is much more appropriate to work with as a log\n",
    "first_tib = df[[\"time_in_business\", \"IDNumber\"]].copy()\n",
    "first_tib[\"log_tib\"] = np.log(first_tib.time_in_business)\n",
    "print(first_tib.log_tib.describe())\n",
    "\n",
    "# Graph it:\n",
    "plt.hist(first_tib.log_tib, bins = 35,\n",
    "         color = 'blue', edgecolor = 'black')\n",
    "plt.axis([0, 20, 0, 1200])\n",
    "plt.xlabel('Log days in business', size = 22)\n",
    "plt.ylabel('# of companies', size= 22)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks much nicer. Let's add it to the dataframe.\n",
    "df[\"log_time_in_business\"] = first_tib[\"log_tib\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should inspect our other variables further, especially since we found some weird values in time_in_business. It's also worth inspecting whether we should log other variables, like the number of directors & officers. However, that's another bit of work I'm going to put off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to learn something. Which companies stay in business longer (ignoring the problem of our censored data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can't dynamically create keywords in Python, so here's the long & ugly way to do this:\n",
    "df = df.assign(contains_realty=pd.Series(False))\n",
    "df = df.assign(contains_construction=pd.Series(False))\n",
    "df = df.assign(contains_investment=pd.Series(False))\n",
    "df = df.assign(contains_auto=pd.Series(False))\n",
    "df = df.assign(contains_cleaning=pd.Series(False))\n",
    "df = df.assign(contains_national=pd.Series(False))\n",
    "df = df.assign(contains_state=pd.Series(False))\n",
    "df = df.assign(contains_electric=pd.Series(False))\n",
    "df = df.assign(contains_health=pd.Series(False))\n",
    "df = df.assign(contains_marine=pd.Series(False))\n",
    "df = df.assign(contains_marketing=pd.Series(False))\n",
    "df = df.assign(contains_restaurant=pd.Series(False))\n",
    "df = df.assign(contains_jewelry=pd.Series(False))\n",
    "df = df.assign(contains_technologies=pd.Series(False))\n",
    "df = df.assign(contains_supply=pd.Series(False))\n",
    "df = df.assign(contains_church=pd.Series(False))\n",
    "df = df.assign(contains_not=pd.Series(False))\n",
    "df = df.assign(contains_new=pd.Series(False))\n",
    "\n",
    "df.contains_realty = df.EntityName.str.contains('realty', case=False, regex=False)\n",
    "df.contains_construction = df.EntityName.str.contains('construction', case=False, regex=False)\n",
    "df.contains_investment = df.EntityName.str.contains('investment', case=False, regex=False)\n",
    "df.contains_auto = df.EntityName.str.contains('auto', case=False, regex=False)\n",
    "df.contains_cleaning = df.EntityName.str.contains('cleaning', case=False, regex=False)\n",
    "df.contains_national = df.EntityName.str.contains('national', case=False, regex=False)\n",
    "df.contains_state = df.EntityName.str.contains('state', case=False, regex=False)\n",
    "df.contains_electric = df.EntityName.str.contains('electric', case=False, regex=False)\n",
    "df.contains_health = df.EntityName.str.contains('health', case=False, regex=False)\n",
    "df.contains_marine = df.EntityName.str.contains('marine', case=False, regex=False)\n",
    "df.contains_marketing = df.EntityName.str.contains('marketing', case=False, regex=False)\n",
    "df.contains_restaurant = df.EntityName.str.contains('restaurant', case=False, regex=False)\n",
    "df.contains_jewelry = df.EntityName.str.contains('jewelry', case=False, regex=False)\n",
    "df.contains_technologies = df.EntityName.str.contains('technologies', case=False, regex=False)\n",
    "df.contains_supply = df.EntityName.str.contains('supply', case=False, regex=False)\n",
    "df.contains_church = df.EntityName.str.contains('church', case=False, regex=False)\n",
    "df.contains_not = df.EntityName.str.contains('not', case=False, regex=False)\n",
    "df.contains_new = df.EntityName.str.contains('new', case=False, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average days in business for different types of businesses (only including businesses that have gone out of business)\n",
    "print(\"Average life expectancy\")\n",
    "print(\"Realty:\", df.loc[df.contains_realty == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Construction:\", df.loc[df.contains_construction == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Investment:\", df.loc[df.contains_investment == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Auto:\", df.loc[df.contains_auto == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Cleaning:\", df.loc[df.contains_cleaning == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"National:\", df.loc[df.contains_national == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"State:\", df.loc[df.contains_state == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Electric:\", df.loc[df.contains_electric == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Health:\", df.loc[df.contains_health == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Marine:\", df.loc[df.contains_marine == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Marketing:\", df.loc[df.contains_marketing == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Restaurant:\", df.loc[df.contains_restaurant == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Jewelry:\", df.loc[df.contains_jewelry == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Technologies:\", df.loc[df.contains_technologies == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Supply:\", df.loc[df.contains_supply == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"Church:\", df.loc[df.contains_church == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"\")\n",
    "print(\"Just for curiosity:\")\n",
    "print(\"Not:\", df.loc[df.contains_not == True, \"time_in_business\"].mean().days, \"days\")\n",
    "print(\"New:\", df.loc[df.contains_new == True, \"time_in_business\"].mean().days, \"days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### With the random sample I used, some differences were substantial. It looks like we may fhave some good predictors.\n",
    "### Let's make the last few variables we're going to use: the number of directors & officers listed, a word-count for the Comments variable similar to what we did for company names, and dummy variables for the type of legal entity registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The D&O data I grabbed isn't very good. I should have saved a count for each row containing data \n",
    "# in the Directors & Officers table, but I grabbed the values instead. \n",
    "# There are lots of clever things we could do, like count the number of words coercible into integers \n",
    "# (1 per street address plus 1 per zip code), count how often \"USA\" shows up as the country code, etc, but\n",
    "# since this isn't my real, final product, I'm going to take the easy way out & count the number of characters\n",
    "# present in the field. That should serve as a fine proxy.\n",
    "df['officers_listed'] = df.apply(lambda row: len(row.d_and_o), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Comments variable is also called \"Purpose\" on Rhode Island's website. It's intended to be where companies describe\n",
    "# what kind of business they intend to do. Most companies give at least a brief answer, but plenty don't.\n",
    "# My favorite common fake response is \"Any Lawful Purpose.\"\n",
    "\n",
    "# Remove all punctuation and newline characters\n",
    "list_of_comments = [row[1] for row in df.itertuples()]\n",
    "list_of_comments_no_punctuation = []\n",
    "for name in list_of_comments:\n",
    "    name = name.replace('\\n', ' ')\n",
    "    list_of_comments_no_punctuation += [re.sub('\\,|\\.|\\!|\\;|\\:|\\?|', '', name)]\n",
    "\n",
    "# Break each name into a list of words so we can count the occurrence of each word\n",
    "new_list = []\n",
    "for words in list_of_comments_no_punctuation:\n",
    "    for word in words.split():\n",
    "        new_list += [word.lower()]\n",
    "\n",
    "# Count the occurrence of each word:\n",
    "name_count = Counter(new_list)\n",
    "popular_names = name_count.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the common & unhelpful words. Look at the resulting list to pick out worthwhile words to focus on.\n",
    "# The number clusters are references to the Rhode Island code, typically Title 7.1, which has been repealed.\n",
    "# There's definitely some good information in which code people are quoting, but it'd take more research to discover that.\n",
    "boring_names = ['inc', 'llc', '&', 'corporation', 'the', 'of', 'company', 'associate', 'co', 'ltd', 'corp', 'and', 'enterprise',\n",
    "               'association', 'group', 'associates', 'enterprises', 'incorporated', 'a', 'limited', 'm', 'r', 'c', 'partnership',\n",
    "               'club', 'in', '', 'd', 'for', 'b', 'lp', 'e', 'h', 'g', 'k', 'p', 'l', '-', 'an', 's', 'title', '7-6', '7-16',\n",
    "               '7-11-51', 'to', '7-11', '7-12-1405', '7-13-8', '7', '7-12-1701', 'as', '116', 'on', 'gl', 'at', '1956',\n",
    "               'business', 'operate']\n",
    "popular_and_not_boring = [item for item in popular_names if item[0] not in boring_names]\n",
    "#print(popular_and_not_boring[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can't dynamically create keywords in Python, so here's the long & ugly way to do this:\n",
    "df = df.assign(comments_contains_retail=pd.Series(False))\n",
    "df = df.assign(comments_contains_management=pd.Series(False))\n",
    "df = df.assign(comments_contains_construction=pd.Series(False))\n",
    "df = df.assign(comments_contains_property=pd.Series(False))\n",
    "df = df.assign(comments_contains_commercial=pd.Series(False))\n",
    "df = df.assign(comments_contains_residential=pd.Series(False))\n",
    "df = df.assign(comments_contains_mortgage=pd.Series(False))\n",
    "df = df.assign(comments_contains_rental=pd.Series(False))\n",
    "df = df.assign(comments_contains_development=pd.Series(False))\n",
    "df = df.assign(comments_contains_restaurant=pd.Series(False))\n",
    "df = df.assign(comments_contains_manage=pd.Series(False))\n",
    "df = df.assign(comments_contains_purchase=pd.Series(False))\n",
    "df = df.assign(comments_contains_food=pd.Series(False))\n",
    "df = df.assign(comments_contains_insurance=pd.Series(False))\n",
    "df = df.assign(comments_contains_equipment=pd.Series(False))\n",
    "df = df.assign(comments_contains_holding=pd.Series(False))\n",
    "df = df.assign(comments_contains_investment=pd.Series(False))\n",
    "df = df.assign(comments_contains_manufacture=pd.Series(False))\n",
    "df = df.assign(comments_contains_design=pd.Series(False))\n",
    "df = df.assign(comments_contains_distribution=pd.Series(False))\n",
    "df = df.assign(comments_contains_maintenance=pd.Series(False))\n",
    "df = df.assign(comments_contains_practice=pd.Series(False))\n",
    "df = df.assign(comments_contains_computer=pd.Series(False))\n",
    "df = df.assign(comments_contains_contracting=pd.Series(False))\n",
    "df = df.assign(comments_contains_health=pd.Series(False))\n",
    "df = df.assign(comments_contains_medical=pd.Series(False))\n",
    "df = df.assign(comments_contains_marketing=pd.Series(False))\n",
    "df = df.assign(comments_contains_installation=pd.Series(False))\n",
    "df = df.assign(comments_contains_jewelry=pd.Series(False))\n",
    "df = df.assign(comments_contains_educational=pd.Series(False))\n",
    "df = df.assign(comments_contains_engineering=pd.Series(False))\n",
    "df = df.assign(comments_contains_community=pd.Series(False))\n",
    "\n",
    "df.comments_contains_retail = df.Comments.str.contains('retail', case=False, regex=False)\n",
    "df.comments_contains_management = df.Comments.str.contains('management', case=False, regex=False)\n",
    "df.comments_contains_construction = df.Comments.str.contains('construction', case=False, regex=False)\n",
    "df.comments_contains_property = df.Comments.str.contains('property', case=False, regex=False)\n",
    "df.comments_contains_development = df.Comments.str.contains('development', case=False, regex=False)\n",
    "df.comments_contains_commercial = df.Comments.str.contains('commercial', case=False, regex=False)\n",
    "df.comments_contains_residential = df.Comments.str.contains('residential', case=False, regex=False)\n",
    "df.comments_contains_mortgage = df.Comments.str.contains('mortgage', case=False, regex=False)\n",
    "df.comments_contains_rental = df.Comments.str.contains('rental', case=False, regex=False)\n",
    "df.comments_contains_restaurant = df.Comments.str.contains('restaurant', case=False, regex=False)\n",
    "df.comments_contains_manage = df.Comments.str.contains('manage', case=False, regex=False)\n",
    "df.comments_contains_purchase = df.Comments.str.contains('purchase', case=False, regex=False)\n",
    "df.comments_contains_food = df.Comments.str.contains('food', case=False, regex=False)\n",
    "df.comments_contains_insurance = df.Comments.str.contains('insurance', case=False, regex=False)\n",
    "df.comments_contains_equipment = df.Comments.str.contains('equipment', case=False, regex=False)\n",
    "df.comments_contains_holding = df.Comments.str.contains('holding', case=False, regex=False)\n",
    "df.comments_contains_investment = df.Comments.str.contains('investment', case=False, regex=False)\n",
    "df.comments_contains_manufacture = df.Comments.str.contains('manufacture', case=False, regex=False)\n",
    "df.comments_contains_design = df.Comments.str.contains('design', case=False, regex=False)\n",
    "df.comments_contains_distribution = df.Comments.str.contains('distribution', case=False, regex=False)\n",
    "df.comments_contains_maintenance = df.Comments.str.contains('maintenance', case=False, regex=False)\n",
    "df.comments_contains_practice = df.Comments.str.contains('practice', case=False, regex=False)\n",
    "df.comments_contains_computer = df.Comments.str.contains('computer', case=False, regex=False)\n",
    "df.comments_contains_contracting = df.Comments.str.contains('contracting', case=False, regex=False)\n",
    "df.comments_contains_health = df.Comments.str.contains('health', case=False, regex=False)\n",
    "df.comments_contains_medical = df.Comments.str.contains('medical', case=False, regex=False)\n",
    "df.comments_contains_marketing = df.Comments.str.contains('marketing', case=False, regex=False)\n",
    "df.comments_contains_installation = df.Comments.str.contains('installation', case=False, regex=False)\n",
    "df.comments_contains_jewelry = df.Comments.str.contains('jewelry', case=False, regex=False)\n",
    "df.comments_contains_educational = df.Comments.str.contains('educational', case=False, regex=False)\n",
    "df.comments_contains_engineering = df.Comments.str.contains('engineering', case=False, regex=False)\n",
    "df.comments_contains_community = df.Comments.str.contains('community', case=False, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for all the types of legal entities that have been registered. We'll use all of them\n",
    "# but one in the analysis ('Foreign Corporation').\n",
    "reg_df = pd.concat([df, pd.get_dummies(df['EntityType'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanatory_variables = ['comments_contains_retail', 'comments_contains_management', 'comments_contains_construction',\n",
    "                        'comments_contains_property', 'comments_contains_development', 'comments_contains_commercial',\n",
    "                        'comments_contains_residential', 'comments_contains_mortgage', 'comments_contains_rental',\n",
    "                        'comments_contains_restaurant', 'comments_contains_manage', 'comments_contains_purchase',\n",
    "                        'comments_contains_food', 'comments_contains_insurance', 'comments_contains_equipment',\n",
    "                        'comments_contains_holding', 'comments_contains_investment', 'comments_contains_manufacture',\n",
    "                        'comments_contains_design', 'comments_contains_distribution', 'comments_contains_maintenance',\n",
    "                        'comments_contains_practice', 'comments_contains_computer', 'comments_contains_contracting',\n",
    "                        'comments_contains_health', 'comments_contains_medical', 'comments_contains_marketing',\n",
    "                        'comments_contains_installation', 'comments_contains_jewelry', 'comments_contains_educational',\n",
    "                        'comments_contains_engineering', 'comments_contains_community',\n",
    "                        'contains_realty', 'contains_construction', 'contains_investment', 'contains_auto', \n",
    "                        'contains_cleaning', 'contains_national', 'contains_state', 'contains_electric', 'contains_health',\n",
    "                        'contains_marine', 'contains_marketing', 'contains_restaurant', 'contains_jewelry',\n",
    "                        'contains_technologies', 'contains_supply', 'contains_church',\n",
    "                        'Bank', 'Credit Union', 'Domestic Limited Liability Company', 'Domestic Limited Liability Partnership',\n",
    "                        'Domestic Limited Partnership', 'Domestic Non-Profit Corporation', 'Domestic Profit Corporation',\n",
    "                        'Foreign Limited Liability Company', 'Foreign Limited Partnership', 'Foreign Non-Profit Corporation',\n",
    "                        'Foreign Registered Limited Liability Partnership', 'Insurance', 'Professional Service Corporation',\n",
    "                        'officers_listed']\n",
    "\n",
    "# time_in_business as days\n",
    "X_train, X_test, y_train, y_test = train_test_split(reg_df[explanatory_variables],\n",
    "                                                    reg_df['time_in_business'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=5)\n",
    "\n",
    "# log_time_in_business\n",
    "X_train_logged, X_test_logged, y_train_logged, y_test_logged = train_test_split(reg_df[explanatory_variables],\n",
    "                                                                reg_df['log_time_in_business'],\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: make some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger question: if I give you details about a new company, can you predict how long they'll be in business?\n",
    "# That's appropriate for a normal linear regression, assuming we ignore the censoring issue for still-in-business companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with a basic elastic net model, modeling time in business in days (not the log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have so many variables here that overfitting is a reasonable concern. Let's go with an elastic net model\n",
    "# as a first line of defense.\n",
    "\n",
    "e_net = sm.OLS(y_train, X_train.astype(float), L1_wt=.5)\n",
    "print(e_net.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is clearly overspecified. Clear out the worst performing variables & see what happens\n",
    "vars_to_keep = []\n",
    "for item in explanatory_variables:\n",
    "    if e_net.fit().pvalues[item] < 0.1:\n",
    "        vars_to_keep += [item]\n",
    "\n",
    "smaller_X_train = X_train[vars_to_keep]\n",
    "\n",
    "e_net2 = sm.OLS(y_train, smaller_X_train.astype(float), L1_wt=.5)\n",
    "print(e_net2.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## An adjusted R^2 of 44% is crazy. How can we know so much about the life expectancy of a company from such a small amount of information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_fit = sm.OLS(y_test, X_test[vars_to_keep].astype(float), L1_wt=.5)\n",
    "\n",
    "#e_fit2 = test_fit.fit()\n",
    "#test_fit = e_fit2.predict(X_test[vars_to_keep])\n",
    "\n",
    "# Set an index for both series\n",
    "#test_fit.reset_index(drop=True, inplace=True)\n",
    "#y_test_indexed = y_test.reset_index(drop=True)\n",
    "\n",
    "# Put them in the same DF\n",
    "#comparison_df = pd.concat([test_fit, y_test_indexed], axis=1)\n",
    "#comparison_df.sort_values(\"time_in_business\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Graph predictions vs actuals. Order by size of the actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use the same elastic net model with logged days_in_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_net_logged = sm.OLS(y_train_logged, X_train_logged.astype(float), L1_wt=.5)\n",
    "print(e_net_logged.fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more, dropping a bunch of poor predictors:\n",
    "vars_to_keep = []\n",
    "for item in explanatory_variables:\n",
    "    if e_net_logged.fit().pvalues[item] < 0.1:\n",
    "        vars_to_keep += [item]\n",
    "\n",
    "smaller_X_train_logged = X_train_logged[vars_to_keep]\n",
    "\n",
    "e_net2_logged = sm.OLS(y_train_logged, smaller_X_train_logged.astype(float), L1_wt=.5)\n",
    "print(e_net2_logged.fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our adjusted R^2 is now 87%. We've done no model tuning whatsoever. I'm astonished.\n",
    "### Also notice that we had 45 worthwhile variables in this last model, compared to 27 in the non-logged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the results from test values against their actual values.\n",
    "\n",
    "#test_fit = sm.OLS(y_test, X_test[vars_to_keep].astype(float), L1_wt=.5)\n",
    "test_fit_logged = sm.OLS(y_test_logged, X_test_logged[vars_to_keep].astype(float), L1_wt=.5)\n",
    "#e_fit2_logged = e_net2_logged.fit()\n",
    "\n",
    "e_fit2_logged = test_fit_logged.fit()\n",
    "test_fit_logged2 = e_fit2.predict(X_test_logged[vars_to_keep])\n",
    "\n",
    "# Set an index for both series\n",
    "test_fit_logged2.reset_index(drop=True, inplace=True)\n",
    "y_test_indexed_logged = y_test.reset_index(drop=True)\n",
    "\n",
    "# Put them in the same DF\n",
    "comparison_df_logged = pd.concat([test_fit_logged2, y_test_indexed_logged], axis=1)\n",
    "comparison_df_logged.sort_values(\"time_in_business\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Graph predictions vs actuals. Order by size of the actual\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(x1, y, 'o', label=\"Data\")\n",
    "#ax.plot(x1, y_true, 'b-', label=\"True\")\n",
    "#ax.plot(np.hstack((x1, x1n)), np.hstack((ypred, ynewpred)), 'r', label=\"OLS prediction\")\n",
    "#ax.legend(loc=\"best\");\n",
    "\n",
    "#fig, ax = plt.subplots()\n",
    "#fig = sm.graphics.plot_fit(test_fit, y_test_indexed, ax=ax)\n",
    "#ax.set_ylabel(\"Murder Rate\")\n",
    "#ax.set_xlabel(\"Poverty Level\")\n",
    "#ax.set_title(\"Linear Regression\")\n",
    "\n",
    "plt.plot(comparison_df_logged[0], comparison_df_logged[\"time_in_business\"], 'ro')\n",
    "plt.axis([0, 15000, 0, 15000])\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish by comparing our linear regression model to Random Forests, where I include the same variables,\n",
    "# then another where I throw in more (all) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is far from a completed study. \n",
    "\n",
    "### Things left to do:\n",
    "    - There's a colinearity problem introduced by having separate variables for keywords in both EntityName and Comments: lots of companies with names like ABC Insurance will also say in their description that they sell insurance. Instead, the keyword variables should apply to both EntityName and Comments.\n",
    "    - Similarly, I should figure out some clever way to make different variables for all words in EntityName & Comments that's present more than X amount of times. This whole inspect-by-hand thing isn't scalable and leaves too much room for my own biases in what names are important.\n",
    "    - Assess the impact of our data being censored (companies that are still in business aren't included in the analysis, which biases our results towards earlier deaths)\n",
    "    - Not all of these businesses failed. Some were sold, merged, etc. An example is this: http://ucc.state.ri.us/CorpWeb/CorpSearch/CorpSummary.aspx?FEIN=000001992. The issue is figuring out the structure of the pages scraped to know how to scrape it.\n",
    "    - Some of the data we used is subject to change over a company's lifetime. Addresses, DBAs, or the number of directors & officers can all change. Rhode Island includes lots of other documents, and it'd be great to see what other data we can get from those.\n",
    "    - Overlay census data based on companies' principal addresses to see how population density or nearby wealth impact their life expectancy\n",
    "    - Use our other variables like stock information, NAICS codes, or which foreign entities use registered agents\n",
    "    - Consider macroeconomic factors: GDP growth, unemployment rates, etc\n",
    "    - Test for unit roots: are there trends over time that we need to account for? Are survival rates different for companies founded in 1900 vs 1980? Do survival rates for different industruies, like technology or construction companies, change significantly over time due to structural changes (not due to macroeconomic factors that we can control for)?\n",
    "    - More data is always better. I only used 10,000 samples in this study.\n",
    "### An extension of this study:\n",
    "    - It's neat to be able to estimate a company's lifespan at birth. It'd be useful to estimate how much longer a company will stay in business, regardless of how old it already is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've done much scraping, then your failed_records list could be pretty long. You don't want to\n",
    "# repeat attempting to scrape those failed records, so save the list to a file.\n",
    "\n",
    "#with open('failed_records', 'wb') as fp:\n",
    "#    pickle.dump(failed_records, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
